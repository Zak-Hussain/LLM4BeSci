{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:51:54.624265Z",
     "start_time": "2023-10-05T08:51:54.596256Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Installing requisite packages\n",
    "    !pip install datasets transformers evaluate\n",
    "    !pip install accelerate -U\n",
    "\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Change working directory to ex1\n",
    "    %cd /content/drive/MyDrive/LLM4behavior_workshop/ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Processing data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cece20f24fb86874"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T09:17:05.898454Z",
     "start_time": "2023-10-24T09:17:04.213691Z"
    }
   },
   "id": "7080dd104c658ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  text     labels\n0    Broken leg. A broken leg (leg fracture) will b...  49.333333\n1    Bulimia. Bulimia is an eating disorder and men...  34.181818\n2    Hyperacusis. Hyperacusis is when everyday soun...  53.818182\n3    DVT. DVT (deep vein thrombosis) is a blood clo...  12.800000\n4    Ectopic pregnancy. An ectopic pregnancy is whe...  31.700000\n..                                                 ...        ...\n772  Typhoid fever. Typhoid fever is a bacterial in...  27.900000\n773  Ankylosing spondylitis. Ankylosing spondylitis...  30.800000\n774  Sleepwalking. Sleepwalking is when someone wal...  71.181818\n775  Fits. If you see someone having a seizure or f...  34.111111\n776  Diabetic retinopathy. Diabetic retinopathy is ...  42.636364\n\n[777 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Broken leg. A broken leg (leg fracture) will b...</td>\n      <td>49.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bulimia. Bulimia is an eating disorder and men...</td>\n      <td>34.181818</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hyperacusis. Hyperacusis is when everyday soun...</td>\n      <td>53.818182</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DVT. DVT (deep vein thrombosis) is a blood clo...</td>\n      <td>12.800000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ectopic pregnancy. An ectopic pregnancy is whe...</td>\n      <td>31.700000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>772</th>\n      <td>Typhoid fever. Typhoid fever is a bacterial in...</td>\n      <td>27.900000</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>Ankylosing spondylitis. Ankylosing spondylitis...</td>\n      <td>30.800000</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>Sleepwalking. Sleepwalking is when someone wal...</td>\n      <td>71.181818</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>Fits. If you see someone having a seizure or f...</td>\n      <td>34.111111</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>Diabetic retinopathy. Diabetic retinopathy is ...</td>\n      <td>42.636364</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the .csv data\n",
    "dat = pd.read_csv('health.csv')\n",
    "dat # Inspecting the data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T09:17:05.925273Z",
     "start_time": "2023-10-24T09:17:05.898681Z"
    }
   },
   "id": "52e0b1bdcee151c9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'labels'],\n    num_rows: 777\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas dataframe to HF Dataset\n",
    "dat = Dataset.from_pandas(dat)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T09:17:05.964171Z",
     "start_time": "2023-10-24T09:17:05.920484Z"
    }
   },
   "id": "d432e8e4833d50b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'Broken leg. A broken leg (leg fracture) will be severely painful and may be swollen or bruised. You usually will not be able to walk on it.If it\\'s a severe fracture, the leg may be an odd shape and the bone may even be poking out of the skin. There may have been a \"crack\" sound when the leg was broken, and the shock and pain of breaking your leg may cause you to feel faint, dizzy or sick.',\n 'labels': 49.33333333}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T09:17:05.964542Z",
     "start_time": "2023-10-24T09:17:05.938298Z"
    }
   },
   "id": "6d624d4d59822c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522, max context length: 512\n"
     ]
    }
   ],
   "source": [
    "# Defining model checkpoint\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "print(f'Vocabulary size: {tokenizer.vocab_size}, max context length: {tokenizer.model_max_length}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T09:17:07.215896Z",
     "start_time": "2023-10-24T09:17:06.899028Z"
    }
   },
   "id": "45565d0aea802193"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/777 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48bc21cf27e94a0dbf96f7b5d184c251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'text': 'Broken leg. A broken leg (leg fracture) will be severely painful and may be swollen or bruised. You usually will not be able to walk on it.If it\\'s a severe fracture, the leg may be an odd shape and the bone may even be poking out of the skin. There may have been a \"crack\" sound when the leg was broken, and the shock and pain of breaking your leg may cause you to feel faint, dizzy or sick.',\n 'labels': 49.33333333,\n 'input_ids': [101,\n  3714,\n  4190,\n  1012,\n  1037,\n  3714,\n  4190,\n  1006,\n  4190,\n  19583,\n  1007,\n  2097,\n  2022,\n  8949,\n  9145,\n  1998,\n  2089,\n  2022,\n  13408,\n  2030,\n  18618,\n  1012,\n  2017,\n  2788,\n  2097,\n  2025,\n  2022,\n  2583,\n  2000,\n  3328,\n  2006,\n  2009,\n  1012,\n  2065,\n  2009,\n  1005,\n  1055,\n  1037,\n  5729,\n  19583,\n  1010,\n  1996,\n  4190,\n  2089,\n  2022,\n  2019,\n  5976,\n  4338,\n  1998,\n  1996,\n  5923,\n  2089,\n  2130,\n  2022,\n  21603,\n  2041,\n  1997,\n  1996,\n  3096,\n  1012,\n  2045,\n  2089,\n  2031,\n  2042,\n  1037,\n  1000,\n  8579,\n  1000,\n  2614,\n  2043,\n  1996,\n  4190,\n  2001,\n  3714,\n  1010,\n  1998,\n  1996,\n  5213,\n  1998,\n  3255,\n  1997,\n  4911,\n  2115,\n  4190,\n  2089,\n  3426,\n  2017,\n  2000,\n  2514,\n  8143,\n  1010,\n  14849,\n  2030,\n  5305,\n  1012,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to tokenize a batch of samples\n",
    "batch_tokenizer = lambda batch: tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "#  Tokenizing the dataset\n",
    "dat = dat.map(batch_tokenizer, batched=True)\n",
    "dat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T12:47:36.790824Z",
     "start_time": "2023-10-23T12:47:36.674668Z"
    }
   },
   "id": "380a264634653886"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 777\n})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting to torch format for input to model\n",
    "dat.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:27.106190Z",
     "start_time": "2023-10-16T10:02:21.965983Z"
    }
   },
   "id": "e4e7cc2aa4ef10cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the model for feature extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f76a69e06ad12da"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # For reproducibility\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:30.298299Z",
     "start_time": "2023-10-16T10:02:30.279733Z"
    }
   },
   "id": "380788ac9d152759"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model and moving it to the GPU if available\n",
    "if torch.cuda.is_available():  # for nvidia GPUs\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # for Apple Metal Performance Sharder (mps) GPUs\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:31.489282Z",
     "start_time": "2023-10-16T10:02:31.454722Z"
    }
   },
   "id": "4d46f6ee9569323"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Model inputs: ['input_ids', 'attention_mask']\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading distilbert-base-uncased and moving it to the GPU if available\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "f'Model inputs: {tokenizer.model_input_names}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:51:56.085319Z",
     "start_time": "2023-10-05T08:51:55.075201Z"
    }
   },
   "id": "fccb948a9c0449da"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/777 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47fd1fff6b7e4889b98e6cd794c0f706"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "torch.Size([777, 768])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(batch):\n",
    "    # Each batch is a dictionary with keys corresponding to the feature names. We only need the input ids and attention masks\n",
    "    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "\n",
    "    # Tell torch not to build the computation graph during inference with `torch.no_grad()`\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state # Extract last hidden states\n",
    "\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "# Extracting features. Features are extracted in batches of 8 samples to avoid running out of memory.\n",
    "dat = dat.map(extract_features, batched=True, batch_size=8)\n",
    "dat['hidden_state'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:19.843954Z",
     "start_time": "2023-10-05T08:51:56.085161Z"
    }
   },
   "id": "4ffde493ddc2580b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting health perception with extracted features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56f3657ebd8b63e1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:19.981497Z",
     "start_time": "2023-10-05T08:52:19.842289Z"
    }
   },
   "id": "74721a900e4b068e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6     \n0   -0.148475 -0.093959 -0.072606 -0.116719 -0.197698 -0.017892  0.583106  \\\n1   -0.188410 -0.126732 -0.108507 -0.293917 -0.218685  0.010479  0.353766   \n2   -0.171668 -0.050836 -0.194084 -0.312468 -0.174100 -0.011414  0.494835   \n3   -0.338618 -0.057856  0.033038 -0.357180 -0.202168  0.036766  0.324142   \n4   -0.098636 -0.401457 -0.182545 -0.179665  0.064003  0.001181  0.291596   \n..        ...       ...       ...       ...       ...       ...       ...   \n772 -0.062923 -0.015915 -0.222807 -0.208136 -0.150941 -0.284561  0.377012   \n773 -0.279977 -0.197014  0.098390 -0.303245 -0.296704  0.008300  0.288116   \n774 -0.073572 -0.369269  0.076614 -0.097476 -0.064454 -0.054622  0.647633   \n775 -0.181972 -0.083371  0.113803 -0.065226  0.033920 -0.130653  0.366495   \n776 -0.148871  0.094196  0.097353 -0.295030  0.079687  0.020537  0.388873   \n\n          7         8         9    ...       758       759       760   \n0    0.199220 -0.294927 -0.524650  ...  0.132813 -0.284638  0.341142  \\\n1    0.414596 -0.126797 -0.443795  ... -0.127295 -0.445322  0.027356   \n2    0.385607 -0.115327 -0.769145  ...  0.038155 -0.398147  0.033521   \n3    0.252396 -0.024016 -0.509216  ...  0.136020 -0.368171  0.198924   \n4    0.541527 -0.020205 -0.360373  ... -0.063380 -0.327838  0.088122   \n..        ...       ...       ...  ...       ...       ...       ...   \n772  0.315895 -0.106533 -0.588246  ...  0.030073 -0.439836  0.044907   \n773  0.194376 -0.128856 -0.353388  ... -0.026887 -0.458937  0.206063   \n774  0.325533 -0.363346 -0.434460  ... -0.008730 -0.349660  0.011828   \n775  0.114534 -0.174858 -0.477767  ... -0.016639 -0.359916  0.018970   \n776  0.140645 -0.075444 -0.502733  ... -0.127892 -0.346318 -0.005479   \n\n          761       762       763       764       765       766       767  \n0   -0.053502  0.222674  0.264824 -0.127856  0.016527 -0.165402  0.297364  \n1   -0.416753 -0.055470  0.358791 -0.196915 -0.088872 -0.025871  0.366596  \n2   -0.147881  0.203515  0.511178 -0.293286 -0.232812 -0.022038  0.263111  \n3   -0.247393  0.010923  0.367897 -0.227436 -0.007300  0.000287  0.664291  \n4   -0.291927 -0.019845  0.092592  0.024323 -0.213633  0.043224  0.738061  \n..        ...       ...       ...       ...       ...       ...       ...  \n772 -0.227704 -0.230707  0.228135 -0.225799  0.025129  0.063843  0.814006  \n773 -0.471932  0.051058  0.154827 -0.420223  0.196191  0.028353  0.592685  \n774 -0.398827 -0.018842  0.212129  0.035592 -0.216679 -0.086199  0.413299  \n775 -0.175317  0.033933  0.393584 -0.225063 -0.109431  0.165128  0.303420  \n776 -0.243680  0.020777  0.164765 -0.174303 -0.099681 -0.098134  0.406878  \n\n[777 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.148475</td>\n      <td>-0.093959</td>\n      <td>-0.072606</td>\n      <td>-0.116719</td>\n      <td>-0.197698</td>\n      <td>-0.017892</td>\n      <td>0.583106</td>\n      <td>0.199220</td>\n      <td>-0.294927</td>\n      <td>-0.524650</td>\n      <td>...</td>\n      <td>0.132813</td>\n      <td>-0.284638</td>\n      <td>0.341142</td>\n      <td>-0.053502</td>\n      <td>0.222674</td>\n      <td>0.264824</td>\n      <td>-0.127856</td>\n      <td>0.016527</td>\n      <td>-0.165402</td>\n      <td>0.297364</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.188410</td>\n      <td>-0.126732</td>\n      <td>-0.108507</td>\n      <td>-0.293917</td>\n      <td>-0.218685</td>\n      <td>0.010479</td>\n      <td>0.353766</td>\n      <td>0.414596</td>\n      <td>-0.126797</td>\n      <td>-0.443795</td>\n      <td>...</td>\n      <td>-0.127295</td>\n      <td>-0.445322</td>\n      <td>0.027356</td>\n      <td>-0.416753</td>\n      <td>-0.055470</td>\n      <td>0.358791</td>\n      <td>-0.196915</td>\n      <td>-0.088872</td>\n      <td>-0.025871</td>\n      <td>0.366596</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.171668</td>\n      <td>-0.050836</td>\n      <td>-0.194084</td>\n      <td>-0.312468</td>\n      <td>-0.174100</td>\n      <td>-0.011414</td>\n      <td>0.494835</td>\n      <td>0.385607</td>\n      <td>-0.115327</td>\n      <td>-0.769145</td>\n      <td>...</td>\n      <td>0.038155</td>\n      <td>-0.398147</td>\n      <td>0.033521</td>\n      <td>-0.147881</td>\n      <td>0.203515</td>\n      <td>0.511178</td>\n      <td>-0.293286</td>\n      <td>-0.232812</td>\n      <td>-0.022038</td>\n      <td>0.263111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.338618</td>\n      <td>-0.057856</td>\n      <td>0.033038</td>\n      <td>-0.357180</td>\n      <td>-0.202168</td>\n      <td>0.036766</td>\n      <td>0.324142</td>\n      <td>0.252396</td>\n      <td>-0.024016</td>\n      <td>-0.509216</td>\n      <td>...</td>\n      <td>0.136020</td>\n      <td>-0.368171</td>\n      <td>0.198924</td>\n      <td>-0.247393</td>\n      <td>0.010923</td>\n      <td>0.367897</td>\n      <td>-0.227436</td>\n      <td>-0.007300</td>\n      <td>0.000287</td>\n      <td>0.664291</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.098636</td>\n      <td>-0.401457</td>\n      <td>-0.182545</td>\n      <td>-0.179665</td>\n      <td>0.064003</td>\n      <td>0.001181</td>\n      <td>0.291596</td>\n      <td>0.541527</td>\n      <td>-0.020205</td>\n      <td>-0.360373</td>\n      <td>...</td>\n      <td>-0.063380</td>\n      <td>-0.327838</td>\n      <td>0.088122</td>\n      <td>-0.291927</td>\n      <td>-0.019845</td>\n      <td>0.092592</td>\n      <td>0.024323</td>\n      <td>-0.213633</td>\n      <td>0.043224</td>\n      <td>0.738061</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>772</th>\n      <td>-0.062923</td>\n      <td>-0.015915</td>\n      <td>-0.222807</td>\n      <td>-0.208136</td>\n      <td>-0.150941</td>\n      <td>-0.284561</td>\n      <td>0.377012</td>\n      <td>0.315895</td>\n      <td>-0.106533</td>\n      <td>-0.588246</td>\n      <td>...</td>\n      <td>0.030073</td>\n      <td>-0.439836</td>\n      <td>0.044907</td>\n      <td>-0.227704</td>\n      <td>-0.230707</td>\n      <td>0.228135</td>\n      <td>-0.225799</td>\n      <td>0.025129</td>\n      <td>0.063843</td>\n      <td>0.814006</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>-0.279977</td>\n      <td>-0.197014</td>\n      <td>0.098390</td>\n      <td>-0.303245</td>\n      <td>-0.296704</td>\n      <td>0.008300</td>\n      <td>0.288116</td>\n      <td>0.194376</td>\n      <td>-0.128856</td>\n      <td>-0.353388</td>\n      <td>...</td>\n      <td>-0.026887</td>\n      <td>-0.458937</td>\n      <td>0.206063</td>\n      <td>-0.471932</td>\n      <td>0.051058</td>\n      <td>0.154827</td>\n      <td>-0.420223</td>\n      <td>0.196191</td>\n      <td>0.028353</td>\n      <td>0.592685</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>-0.073572</td>\n      <td>-0.369269</td>\n      <td>0.076614</td>\n      <td>-0.097476</td>\n      <td>-0.064454</td>\n      <td>-0.054622</td>\n      <td>0.647633</td>\n      <td>0.325533</td>\n      <td>-0.363346</td>\n      <td>-0.434460</td>\n      <td>...</td>\n      <td>-0.008730</td>\n      <td>-0.349660</td>\n      <td>0.011828</td>\n      <td>-0.398827</td>\n      <td>-0.018842</td>\n      <td>0.212129</td>\n      <td>0.035592</td>\n      <td>-0.216679</td>\n      <td>-0.086199</td>\n      <td>0.413299</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>-0.181972</td>\n      <td>-0.083371</td>\n      <td>0.113803</td>\n      <td>-0.065226</td>\n      <td>0.033920</td>\n      <td>-0.130653</td>\n      <td>0.366495</td>\n      <td>0.114534</td>\n      <td>-0.174858</td>\n      <td>-0.477767</td>\n      <td>...</td>\n      <td>-0.016639</td>\n      <td>-0.359916</td>\n      <td>0.018970</td>\n      <td>-0.175317</td>\n      <td>0.033933</td>\n      <td>0.393584</td>\n      <td>-0.225063</td>\n      <td>-0.109431</td>\n      <td>0.165128</td>\n      <td>0.303420</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>-0.148871</td>\n      <td>0.094196</td>\n      <td>0.097353</td>\n      <td>-0.295030</td>\n      <td>0.079687</td>\n      <td>0.020537</td>\n      <td>0.388873</td>\n      <td>0.140645</td>\n      <td>-0.075444</td>\n      <td>-0.502733</td>\n      <td>...</td>\n      <td>-0.127892</td>\n      <td>-0.346318</td>\n      <td>-0.005479</td>\n      <td>-0.243680</td>\n      <td>0.020777</td>\n      <td>0.164765</td>\n      <td>-0.174303</td>\n      <td>-0.099681</td>\n      <td>-0.098134</td>\n      <td>0.406878</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame(dat['hidden_state'])\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:19.998926Z",
     "start_time": "2023-10-05T08:52:19.982368Z"
    }
   },
   "id": "e27dfbd089027633"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'Train size: 621, test size: 156'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing ridge regression \n",
    "regr = RidgeCV()\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, dat['labels'], test_size=.2, random_state=42)\n",
    "f'Train size: {len(X_train)}, test size: {len(X_test)}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:20.022990Z",
     "start_time": "2023-10-05T08:52:19.999930Z"
    }
   },
   "id": "a1ed81c07ac83c1"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'Test R2 = 0.54'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model and evaluating performance\n",
    "regr.fit(X_train, y_train)\n",
    "f'Test R2 = {regr.score(X_test, y_test).round(2)}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:20.161002Z",
     "start_time": "2023-10-05T08:52:20.014069Z"
    }
   },
   "id": "6dab163402967d73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting health perception with fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9781773b6a3599c5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:38.102036Z",
     "start_time": "2023-10-16T10:02:35.215060Z"
    }
   },
   "id": "407774333bbb12b8"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 621\n    })\n    test: Dataset({\n        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 156\n    })\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test sets\n",
    "dat = dat.train_test_split(test_size=.2, seed=42)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:38.732344Z",
     "start_time": "2023-10-16T10:02:38.723747Z"
    }
   },
   "id": "d334c67f17f6c467"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "datasets.arrow_dataset.Dataset"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dat['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T10:02:50.808317Z",
     "start_time": "2023-10-16T10:02:50.805605Z"
    }
   },
   "id": "382657be3ce30c3b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading distilbert-base-uncased and moving it to the GPU if available\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=1) # num_labels=1 for regression\n",
    "         .to(device))\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:52:20.834342Z",
     "start_time": "2023-10-05T08:52:20.171327Z"
    }
   },
   "id": "7bab8ccda81a8dcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhussain/opt/anaconda3/envs/LLM4JDM/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting up training arguments for the trainer\n",
    "model_name = f\"{model_ckpt}-finetuned-health\"\n",
    "batch_size = 8\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,  # output directory to save training checkpoints\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_strategy=\"epoch\", # log training metrics at every epoch\n",
    "    evaluation_strategy=\"epoch\", # evaluate at the end of every epoch\n",
    "    num_train_epochs=10, # number of times to iterate over the training data\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Computes the coefficient of determination (R2) on the test set\"\"\"\n",
    "    metric = evaluate.load(\"r_squared\")\n",
    "    preds, labels = eval_preds\n",
    "    return {\"r_squared\": metric.compute(predictions=preds, references=labels)}\n",
    "\n",
    "\n",
    "# Instantiating the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dat['train'],\n",
    "    eval_dataset=dat['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-05T11:08:29.223260Z"
    }
   },
   "id": "60d4a085896ea9ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3541e77e73d80c93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
